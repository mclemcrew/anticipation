model:
  type: gpt2
  hidden_dim: 1024
  num_heads: 16
  num_layers: 24
  seq_len: 1024
  scale_attn_by_inverse_layer_idx: true
  gradient_checkpointing: true
  use_flash_attention: false

override_vocab_size: 55028
output_dir: /checkpoints/your-checkpoint/hf
checkpoint_path: /checkpoints/your-checkpoint
save_tokenizer: false